{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_INSTANCE_ID = \"88b283dbda20477bb588e07f0db4d0c8\"\n",
    "N_CPUS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bogul/scalarizing/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ujson module not found, using json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from copy import deepcopy\n",
    "\n",
    "from distributed import Client\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import RunStatus\n",
    "from distributed import get_client\n",
    "\n",
    "import ray\n",
    "from tqdm.auto import tqdm\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.optimize import minimize\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pymoo.core.problem import DaskParallelization\n",
    "\n",
    "from scalarizing.scalarizing import FindingBestExpressionSingleDatasetProblem, FindingBestExpressionProblemMutation, \\\n",
    "    FindingBestExpressionProblemCrossover, FindingBestExpressionProblemSampling, scorer_creator\n",
    "from scalarizing.scoring_functions import default_scoring_function, diversity_metric_scoring_function\n",
    "from scalarizing.utils import top_n_indicies\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from box import Box\n",
    "from sklearn.linear_model import Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///home/bogul/scalarizing/notebooks/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.start_run(run_id=EXPERIMENT_INSTANCE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Box(run.data.params, box_recast={\n",
    "                'bagging_size': int,\n",
    "                'ensemble_size': int,\n",
    "                'n_gen': int,\n",
    "                'pop_size' : int\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.train_path = f\"/home/bogul/scalarizing/notebooks/{params.train_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Box: {'bagging_size': 500, 'ensemble_size': 20, 'n_gen': 100, 'pop_size': 100, 'scoring_method': 'normal', 'train_path': '/home/bogul/scalarizing/notebooks/../../datasets/processed/wdbc-train-1-s2.csv', 'dataset': 'wdbc-1-s2.csv'}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data.drop('TARGET', axis=1).values\n",
    "    y = data['TARGET'].values\n",
    "\n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"y\": y\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = params.train_path\n",
    "test_path = train_path.replace('train','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bogul/scalarizing/notebooks/../../datasets/processed/wdbc-train-1-s2.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bogul/scalarizing/notebooks/../../datasets/processed/wdbc-test-1-s2.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del params['train_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Box({\n",
    "        'train': read_dataset(train_path),\n",
    "        'test': read_dataset(test_path),\n",
    "        'name': train_path.split(\"/\")[-1].replace(\"-train\", '')\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class predict_wrapper(object):\n",
    "    def __init__(self, predict_func, labels):\n",
    "        self.predict_func = predict_func\n",
    "        self.labels = labels\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.labels[self.predict_func(*args, **kwargs)]\n",
    "\n",
    "def raise_not_implemented():\n",
    "    raise NotImplemented(\"Predict proba is not supported\")\n",
    "def extract_classifiers_from_bagging(bagging):\n",
    "\n",
    "    extracted = []\n",
    "    for classifier in bagging.estimators_:\n",
    "        cloned_classifier = deepcopy(classifier)\n",
    "        cloned_classifier.predict = predict_wrapper(cloned_classifier.predict, bagging.classes_)\n",
    "        cloned_classifier.predict_proba = raise_not_implemented\n",
    "\n",
    "        extracted.append(cloned_classifier)\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def execute_in_ray(f, x):\n",
    "    return f(x)\n",
    "\n",
    "class RayParallelization:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, f, X):\n",
    "        results = [execute_in_ray.remote(f, x) for x in X]\n",
    "\n",
    "        return ray.get(results)\n",
    "\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ExecutorParallelization:\n",
    "\n",
    "    def __init__(self, executor) -> None:\n",
    "        super().__init__()\n",
    "        self.executor = executor\n",
    "\n",
    "    def __call__(self, f, X):\n",
    "        jobs = [self.executor.submit(f, x) for x in X]\n",
    "        return [job.result() for job in jobs]\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state.pop(\"executor\", None) # is not serializable\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level='INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadasdas\n"
     ]
    }
   ],
   "source": [
    "a = {\n",
    " \"c\": lambda abc: print(abc)\n",
    "\n",
    "}\n",
    "\n",
    "Box(a).c(\"sadasdas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_run_experiment(dataset, params, run_id, scoring_function=default_scoring_function, parallelization=RayParallelization(), mlflow_client = MlflowClient()):\n",
    "    from loguru import logger\n",
    "\n",
    "    bagging = BaggingClassifier(base_estimator=Perceptron(), n_estimators=params.bagging_size, max_samples=0.3, random_state=42)\n",
    "    bagging.fit(dataset.train.x, dataset.train.y)\n",
    "    problem = FindingBestExpressionSingleDatasetProblem(dataset.train, extract_classifiers_from_bagging(bagging), ensemble_size=params.ensemble_size, scoring_function=scoring_function, elementwise_runner=parallelization)\n",
    "    result = minimize(problem,\n",
    "                      GA(\n",
    "                          pop_size=params.pop_size,\n",
    "                          verbose=True,\n",
    "                          seed=42,\n",
    "                          eliminate_duplicates=False,\n",
    "                          mutation=FindingBestExpressionProblemMutation(),\n",
    "                          crossover=FindingBestExpressionProblemCrossover(),\n",
    "                          sampling=FindingBestExpressionProblemSampling()\n",
    "                      ),\n",
    "                      (\"n_gen\", params.n_gen),\n",
    "                      verbose=False,\n",
    "                      save_history=False,\n",
    "                      seed=42)\n",
    "\n",
    "    bagging_estimators = np.array(extract_classifiers_from_bagging(bagging))\n",
    "\n",
    "    scorer = scorer_creator(result.X[0], labels=np.unique(dataset.train.y))\n",
    "    estimator_accuracies = []\n",
    "    estimator_scores = []\n",
    "\n",
    "    for estimator in bagging_estimators:\n",
    "        predictions = estimator.predict(dataset.train.x)\n",
    "\n",
    "        estimator_accuracies.append(accuracy_score(dataset.train.y, predictions))\n",
    "        estimator_scores.append(scorer(dataset.train.y, predictions))\n",
    "\n",
    "    estimators_selected_by_accuracy = bagging_estimators[top_n_indicies(estimator_accuracies, params.ensemble_size)]\n",
    "    estimators_selected_by_score = bagging_estimators[top_n_indicies(estimator_scores, params.ensemble_size)]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ensemble = EnsembleVoteClassifier(clfs=estimators_selected_by_accuracy,\n",
    "                                      weights=[1 for _ in range(params.ensemble_size)],\n",
    "                                      fit_base_estimators=False)\n",
    "\n",
    "    ensemble.fit(dataset.train.x, dataset.train.y) # Required by design, but does nothing apart from checking labels\n",
    "    accuracy_ensemble_train_accuracy = accuracy_score(dataset.train.y, ensemble.predict(dataset.train.x))\n",
    "    accuracy_ensemble_accuracy = accuracy_score(dataset.test.y, ensemble.predict(dataset.test.x))\n",
    "\n",
    "\n",
    "    ensemble = EnsembleVoteClassifier(clfs=estimators_selected_by_score,\n",
    "                                      weights=[1 for _ in range(params.ensemble_size)],\n",
    "                                      fit_base_estimators=False)\n",
    "\n",
    "    ensemble.fit(dataset.train.x, dataset.train.y) # Required by design, but does nothing apart from checking labels\n",
    "    score_ensemble_train_accuracy = accuracy_score(dataset.train.y, ensemble.predict(dataset.train.x))\n",
    "    score_ensemble_accuracy = accuracy_score(dataset.test.y, ensemble.predict(dataset.test.x))\n",
    "\n",
    "\n",
    "    if accuracy_ensemble_train_accuracy > score_ensemble_train_accuracy:\n",
    "        mlflow_client.log_metric(run_id, \"accuracy_ensemble_selected\", True)\n",
    "        selected_ensemble_accuracy = accuracy_ensemble_accuracy\n",
    "    else:\n",
    "        \n",
    "        mlflow_client.log_metric(run_id, \"accuracy_ensemble_selected\", False)\n",
    "        selected_ensemble_accuracy = score_ensemble_accuracy\n",
    "\n",
    "    mlflow_client.log_metric(run_id, \"selected_ensemble_accuracy\", selected_ensemble_accuracy)\n",
    "    mlflow_client.log_metric(run_id, \"method_selection_accuracy\", score_ensemble_accuracy)\n",
    "    mlflow_client.log_metric(run_id, \"accuracy_selection_accuracy\", accuracy_ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(dataset, params, run_id, parallelization=RayParallelization()):\n",
    "    scoring_function = default_scoring_function\n",
    "    \n",
    "    if params.scoring_method == 'diversity':\n",
    "        scoring_function = diversity_metric_scoring_function\n",
    "    \n",
    "    \n",
    "    from loguru import logger\n",
    "    mlflow_client = MlflowClient()\n",
    "\n",
    "    mlflow_client.log_param(run_id, \"dataset\", dataset.name)\n",
    "\n",
    "    try:\n",
    "        do_run_experiment(dataset, params, scoring_function=scoring_function, run_id=run_id, parallelization=parallelization)\n",
    "        mlflow_client.set_terminated(run_id=run_id)\n",
    "    except Exception as ex:\n",
    "        logger.exception(ex)\n",
    "        mlflow_client.set_terminated(run_id=run_id, status=\"FAILED\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init(ignore_reinit_error=True, num_cpus=N_CPUS)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "run_experiment(dataset, Box({**params, 'n_gen': 1}), EXPERIMENT_INSTANCE_ID, parallelization=RayParallelization())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
